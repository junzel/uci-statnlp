{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classify\n",
    "import numpy as np\n",
    "\n",
    "from speech import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Logistic Regression + Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "train.tsv\n",
      "4370\n",
      "-- dev data\n",
      "dev.tsv\n",
      "414\n",
      "-- transforming data and labels\n",
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"data/speech.tar.gz\"\n",
    "speech = read_files(tarfname)\n",
    "print(\"Training classifier\")\n",
    "import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {\n",
    "#         'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#         'tfidf__use_idf': (True, False),\n",
    "#         'clf__alpha': (1e-2, 1e-3),\n",
    "#     }\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# cls = LogisticRegression(random_state=0, penalty='l2', solver='lbfgs', tol=1e-6, max_iter=300, multi_class='multinomial')\n",
    "# cls.fit(speech.trainX, speech.trainy)\n",
    "\n",
    "def train_with_params(X, y, random_state=0, penalty='l2', solver='lbfgs', tol=1e-2, max_iter=1000, multi_class='auto', C=1.0): \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    cls = LogisticRegression(random_state=random_state, penalty=penalty, solver=solver, tol=tol, max_iter=max_iter, multi_class=multi_class)\n",
    "    cls.fit(X, y)\n",
    "    return cls\n",
    "\n",
    "# list_C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "# for C in list_C:\n",
    "#     cls = train_with_params(speech.trainX, speech.trainy, C=C)\n",
    "#     print(C)\n",
    "#     classify.evaluate(speech.trainX, speech.trainy, cls)\n",
    "#     classify.evaluate(speech.devX, speech.devy, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "train.tsv\n",
      "4370\n",
      "-- dev data\n",
      "dev.tsv\n",
      "414\n",
      "-- transforming data and labels\n",
      "Training classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junzel/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/junzel/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n",
      "  Accuracy 0.9832951945080092\n",
      "  Accuracy 0.4082125603864734\n"
     ]
    }
   ],
   "source": [
    "cls = classify.train_classifier(speech.trainX, speech.trainy)\n",
    "print(\"Evaluating\")\n",
    "classify.evaluate(speech.trainX, speech.trainy, cls)\n",
    "classify.evaluate(speech.devX, speech.devy, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.trainX.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACHMANN_PRIMARY2012', 'BIDEN_PRIMARY2008', 'CAIN_PRIMARY2012',\n",
       "       'CLINTON_PRIMARY2008', 'EDWARDS_PRIMARY2008',\n",
       "       'GINGRICH_PRIMARY2012', 'GIULIANI_PRIMARY2008',\n",
       "       'HUCKABEE_PRIMARY2008', 'HUNTSMAN_PRIMARY2012',\n",
       "       'MCCAIN_PRIMARY2008', 'OBAMA_PRIMARY2008', 'PAUL_PRIMARY2012',\n",
       "       'PAWLENTY_PRIMARY2012', 'PERRY_PRIMARY2012',\n",
       "       'RICHARDSON_PRIMARY2008', 'ROMNEY_PRIMARY2008',\n",
       "       'ROMNEY_PRIMARY2012', 'SANTORUM_PRIMARY2012',\n",
       "       'THOMPSON_PRIMARY2008'], dtype='<U22')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-75-c478b255d383>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-c478b255d383>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    #         print(key)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "voc_dict = speech.count_vect.vocabulary_\n",
    "for key in voc_dict.keys():\n",
    "    if voc_dict[key] > 7000:\n",
    "#         print(voc_dict[key])\n",
    "#         print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=414)\n",
    "trainX_pca = pca.fit_transform(speech.trainX.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4370, 7916)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 7916)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.devX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy 0.5125858123569794\n",
      "  Accuracy 0.16183574879227053\n"
     ]
    }
   ],
   "source": [
    "cls = train_with_params(pca.fit_transform(speech.trainX.toarray()), speech.trainy, max_iter=1000, tol=1e-9)\n",
    "classify.evaluate(pca.fit_transform(speech.trainX.toarray()), speech.trainy, cls)\n",
    "classify.evaluate(pca.fit_transform(speech.devX.toarray()), speech.devy, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize so-far observations\n",
    "## Tfidf\n",
    "Using plain CountVectorizer, it is super easy to overfit to the training data. And development accuracy doesn't improve much with respect to different hyper-paramter settings.\n",
    "Using TfidfVectorizer, the logistic regression model is hard to overfit on the training data (with penalty term). Give the best performance when setting penalty of logistic regression as 'none'.\n",
    "## ngram\n",
    "(1,2) gives the best development accuracy. (1,1) or (1,3) doesn't work as well as (1,2). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
